{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fragmentación de texto con base en el contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se ha extraído el texto de un documento, es posible que se requiera dividirlo en fragmentos más pequeños, como oraciones o palabras, para su posterior análisis. En este notebook se presentan algunas técnicas para realizar esta tarea con base en el contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from seaborn) (2.4.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from seaborn) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     Language,\n\u001b[32m      9\u001b[39m     MarkdownHeaderTextSplitter,\n\u001b[32m     10\u001b[39m     RecursiveCharacterTextSplitter,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlangchain_docs_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangchainDocsLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.schema'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tiktoken\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import (\n",
    "    Language,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "from src.langchain_docs_loader import LangchainDocsLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos la función `print_example_doc_splits_from_docs` a través de este notebook para imprimir el contenido de los documentos cuya metadata `source` coincida con el valor proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example_doc_splits_from_docs(\n",
    "    docs: list[Document],\n",
    "    source: str,\n",
    ") -> None:\n",
    "    for doc in docs:\n",
    "        if doc.metadata[\"source\"] == source:\n",
    "            print(\"\\n\")\n",
    "            print(f\" {doc.metadata['source']} \".center(80, \"=\"))\n",
    "            print(\"\\n\")\n",
    "            print(doc.page_content)\n",
    "\n",
    "\n",
    "print_split_for_lcle = partial(\n",
    "    print_example_doc_splits_from_docs,\n",
    "    source=\"https://python.langchain.com/docs/expression_language/interface\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "De aquí en adelante, utilizaremos el conjunto de documentos extraídos en el notebook [01_context_aware_text_extraction.ipynb](01_context_aware_text_extraction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = LangchainDocsLoader(include_output_cells=True)\n",
    "docs = loader.load()\n",
    "f\"Loaded {len(docs)} documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragmentación de texto sin tener en cuenta el contexto\n",
    "\n",
    "La forma más sencilla de fragmentar texto es utilizando la función `split` de Python. Esta función recibe como parámetro un caracter o cadena de caracteres que se utilizará como separador. Por ejemplo, para fragmentar un texto en oraciones, se puede utilizar el caracter `.` como separador.\n",
    "\n",
    "Sin embargo, podemos ir un poco más allá y utilizar `RecursiveCharacterTextSplitter()` de langchain para dividir texto observando caracteres de forma recursiva. Esta herramienta intenta, de manera recursiva, dividir el texto por diferentes caracteres para encontrar uno que funcione, permitiendo así una fragmentación de texto más precisa y adaptable a diferentes contextos y formatos de texto, aunque no tenga en cuenta el contexto semántico del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framentación de texto con contexto de distrubución de `tokens`\n",
    "\n",
    "En muchas ocasiones, el contexto de cómo se distribuyen los `tokens` o `caracteres` en el texto puede ser de gran ayuda para decidir cómo fragmentar el texto. Veámoslo con un ejemplo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de apoyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def num_tokens_from_document(\n",
    "    document: Document, encoding_name: str = \"cl100k_base\"\n",
    ") -> int:\n",
    "    \"\"\"Returns the number of tokens in a document.\"\"\"\n",
    "    return num_tokens_from_string(document.page_content, encoding_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas de tokens en los textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos algunas estadísticas de los `tokens` en los textos utilizado `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_document = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La amplia variabilidad en el número de tokens por documento sugiere que se está tratando con documentos de longitudes muy diversas, desde muy cortos hasta muy largos. Esto podría afectar a los análisis subsiguientes y debería tenerse en cuenta al desarrollar modelos de procesamiento de lenguaje natural, ajustando posiblemente los métodos de preprocesamiento o utilizando técnicas que puedan manejar eficientemente documentos de diferentes longitudes. Además, el sesgo a la derecha en la distribución sugiere que aunque la mayoría de los documentos son relativamente cortos, hay algunos documentos extremadamente largos que podrían ser atípicos y necesitar un tratamiento especial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de distribución de tokens sin outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = tokens_per_document.quantile(0.25)\n",
    "Q3 = tokens_per_document.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# We only filter outliers by upper bound since we don't have problems with short documents.\n",
    "filtered_tokens = tokens_per_document[(tokens_per_document <= upper_bound)]\n",
    "\n",
    "# Plot the important sections of the histogram\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig = sns.histplot(filtered_tokens, kde=True)\n",
    "fig.set(\n",
    "    xlabel=\"Number of tokens\",\n",
    "    ylabel=\"Number of documents\",\n",
    "    title=\"Number of tokens per document\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framentación de texto con base en la distribución de tokens\n",
    "\n",
    "Considerando los resultados anteriores, podemos utilizar la información de la distribución de tokens para fragmentar el texto de forma más precisa. Para ello, utilizaremos la función `RecursiveCharacterTextSplitter()` de langchain, pero ahora especifícaremos los parámetros `chunk_size` y `chunk_overlap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=,\n",
    "    chunk_overlap=,\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "\n",
    "splitted_with_little_context = splitter.split_documents(docs)\n",
    "print_split_for_lcle(splitted_with_little_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framentación de texto con contexto\n",
    "\n",
    "Con el dominio del problema podemos fragmentar el texto de manera más precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framentación de texto con base en una especificación de lenguaje como contexto\n",
    "\n",
    "En nuestro ejemplo, nuestros documentos son `Markdown`, por lo que podríamos fragmentar el documento en función de los caracteres que se utilizan para definir los encabezados de las secciones y otros elementos de formato.\n",
    "\n",
    "En este caso, la función internamente utiliza los siguientes patrones para fragmentar el texto:\n",
    "\n",
    "```python\n",
    "[\n",
    "    # First, try to split along Markdown headings (starting with level 2)\n",
    "    \"\\n#{1,6} \",\n",
    "    # Note the alternative syntax for headings (below) is not handled here\n",
    "    # Heading level 2\n",
    "    # ---------------\n",
    "    # End of code block\n",
    "    \"```\\n\",\n",
    "    # Horizontal lines\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    # Note that this splitter doesn't handle horizontal lines defined\n",
    "    # by *three or more* of ***, ---, or ___, but this is not handled\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_language_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "md_language_splits = md_language_splitter.split_documents(docs)\n",
    "print_split_for_lcle(md_language_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framentación de texto utilizando encabezados como contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En contrastraste con el ejemplo anterior, en este caso utilizaremos únicamente los encabezados de los documentos como contexto para fragmentar el texto. Estos encabezados pasarán a formar parte de los meta-datos de los fragmentos.\n",
    "\n",
    "Dentro de cada framento de encabezado, podríamos repetir el proceso de fragmentación de texto con base en la distribución de tokens o en una especificación de lenguaje como contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_headers_splits: list[Document] = []\n",
    "\n",
    "for doc in docs:\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "        ]\n",
    "    )\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.MARKDOWN,\n",
    "        chunk_size=1000,  # try then with 150\n",
    "        chunk_overlap=50,\n",
    "        length_function=num_tokens_from_string,\n",
    "    )\n",
    "\n",
    "    splits = md_header_splitter.split_text(doc.page_content)\n",
    "\n",
    "    splits = text_splitter.split_documents(splits)\n",
    "    splits = [\n",
    "        Document(\n",
    "            page_content=split.page_content,\n",
    "            metadata={\n",
    "                **split.metadata,\n",
    "                **doc.metadata,\n",
    "            },\n",
    "        )\n",
    "        for split in splits\n",
    "    ]\n",
    "    md_headers_splits.extend(splits)\n",
    "\n",
    "print_split_for_lcle(md_headers_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "platzi_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
