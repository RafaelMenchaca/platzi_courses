{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de texto con base en el contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: HTML2Text in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (2025.4.15)\n",
      "Requirement already satisfied: langchain in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain) (1.2.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (0.6.4)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (25.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.14.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\angel\\documents\\platzi_courses\\platzi_venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 HTML2Text -U langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "from functools import partial\n",
    "from typing import Generator\n",
    "from bs4 import BeautifulSoup, Doctype, NavigableString, SoupStrainer, Tag\n",
    "from dotenv import load_dotenv\n",
    "from html2text import HTML2Text\n",
    "from IPython.core.display import Markdown\n",
    "from langchain_community.document_loaders import DocugamiLoader, RecursiveUrlLoader\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset y función de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_url = \"https://python.langchain.com/docs/get_started/quickstart\"\n",
    "\n",
    "load_documents = partial(\n",
    "    RecursiveUrlLoader,\n",
    "    url=doc_url,\n",
    "    max_depth=3,\n",
    "    prevent_outside=True,\n",
    "    check_response_status=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de texto sin tener en cuenta el contexto\n",
    "\n",
    "La primera aproximación para extraer texto de una página web es simplemente obtener el texto de todos los elementos de la página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de texto teniendo un poco de contexto\n",
    "\n",
    "El texto de la documentación de `Langchain` está escrito en `Markdown` y por lo tanto, tiene una estructura que puede ser aprovechada para extraer el texto de manera más precisa.\n",
    "\n",
    "Utilicemos una librería que nos permita convertir el texto de `HTML` a `Markdown` y así poder extraer el texto de manera más precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de texto teniendo en cuenta el contexto\n",
    "\n",
    "Si bien, cuando utilizamos una libreraía para convertir el texto de `HTML` a `Markdown` pudimos extraer el texto de manera más precisa, aún hay algunos casos en los que no se logra extraer el texto de manera correcta.\n",
    "\n",
    "Es aquí donde entra en juego el dominio del problema. Con base en el conocimiento que tenemos del problema, podemos crear una función que nos permita extraer el texto de manera más precisa.\n",
    "\n",
    "Imagina que `langchain_docs_extractor` es como un obrero especializado en una fábrica cuyo trabajo es transformar materias primas (documentos HTML) en un producto terminado (un string limpio y formateado). Este obrero usa una herramienta especial, `get_text`, como una máquina para procesar las materias primas en piezas utilizables, examinando cada componente de la materia prima **pieza por pieza**, y usa el mismo proceso repetidamente (**recursividad**) para descomponer los componentes en su forma más simple. Al final, ensambla todas las piezas procesadas en un producto completo y hace algunos refinamientos finales antes de que el producto salga de la fábrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_docs_extractor(\n",
    "    html: str,\n",
    "    include_output_cells: bool,\n",
    "    path_url: str | None = None,\n",
    ") -> str:\n",
    "    soup = BeautifulSoup(\n",
    "        html,\n",
    "        \"lxml\",\n",
    "        parse_only=SoupStrainer(name=\"article\"),\n",
    "    )\n",
    "\n",
    "    # Remove all the tags that are not meaningful for the extraction.\n",
    "    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n",
    "    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n",
    "\n",
    "    # get_text() method returns the text of the tag and all its children.\n",
    "    def get_text(tag: Tag) -> Generator[str, None, None]:\n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Doctype):\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, NavigableString):\n",
    "                yield child.get_text()\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "                    text = child.get_text(strip=False)\n",
    "\n",
    "                    if text == \"API Reference:\":\n",
    "                        yield f\"> **{text}**\\n\"\n",
    "                        ul = child.find_next_sibling(\"ul\")\n",
    "                        if ul is not None and isinstance(ul, Tag):\n",
    "                            ul.attrs[\"api_reference\"] = \"true\"\n",
    "                    else:\n",
    "                        yield f\"{'#' * int(child.name[1:])} \"\n",
    "                        yield from child.get_text(strip=False)\n",
    "\n",
    "                        if path_url is not None:\n",
    "                            link = child.find(\"a\")\n",
    "                            if link is not None:\n",
    "                                yield f\" [](/{path_url}/{link.get('href')})\"\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"a\":\n",
    "                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n",
    "                elif child.name == \"img\":\n",
    "                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n",
    "                elif child.name in [\"strong\", \"b\"]:\n",
    "                    yield f\"**{child.get_text(strip=False)}**\"\n",
    "                elif child.name in [\"em\", \"i\"]:\n",
    "                    yield f\"_{child.get_text(strip=False)}_\"\n",
    "                elif child.name == \"br\":\n",
    "                    yield \"\\n\"\n",
    "                elif child.name == \"code\":\n",
    "                    parent = child.find_parent()\n",
    "                    if parent is not None and parent.name == \"pre\":\n",
    "                        classes = parent.attrs.get(\"class\", \"\")\n",
    "\n",
    "                        language = next(\n",
    "                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n",
    "                            None,\n",
    "                        )\n",
    "                        if language is None:\n",
    "                            language = \"\"\n",
    "                        else:\n",
    "                            language = language.split(\"-\")[1]\n",
    "\n",
    "                        if language in [\"pycon\", \"text\"] and not include_output_cells:\n",
    "                            continue\n",
    "\n",
    "                        lines: list[str] = []\n",
    "                        for span in child.find_all(\"span\", class_=\"token-line\"):\n",
    "                            line_content = \"\".join(\n",
    "                                token.get_text() for token in span.find_all(\"span\")\n",
    "                            )\n",
    "                            lines.append(line_content)\n",
    "\n",
    "                        code_content = \"\\n\".join(lines)\n",
    "                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n",
    "                    else:\n",
    "                        yield f\"`{child.get_text(strip=False)}`\"\n",
    "\n",
    "                elif child.name == \"p\":\n",
    "                    yield from get_text(child)\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ul\":\n",
    "                    if \"api_reference\" in child.attrs:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"> - \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    else:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"- \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ol\":\n",
    "                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n",
    "                        yield f\"{i + 1}. \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n",
    "                    \"class\", [\"\"]\n",
    "                ):\n",
    "                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n",
    "                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n",
    "                    for tab, tab_panel in zip(tabs, tab_panels):\n",
    "                        tab_name = tab.get_text(strip=True)\n",
    "                        yield f\"{tab_name}\\n\"\n",
    "                        yield from get_text(tab_panel)\n",
    "                elif child.name == \"table\":\n",
    "                    thead = child.find(\"thead\")\n",
    "                    header_exists = isinstance(thead, Tag)\n",
    "                    if header_exists:\n",
    "                        headers = thead.find_all(\"th\")\n",
    "                        if headers:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(header.get_text() for header in headers)\n",
    "                            yield \" |\\n\"\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\"----\" for _ in headers)\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    tbody = child.find(\"tbody\")\n",
    "                    tbody_exists = isinstance(tbody, Tag)\n",
    "                    if tbody_exists:\n",
    "                        for row in tbody.find_all(\"tr\"):\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name in [\"button\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    yield from get_text(child)\n",
    "\n",
    "    joined = \"\".join(get_text(soup))\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo de salida es ahora en formato Markdown, lo que permite visualizarlo en cualquier editor de texto o en GitHub, ofreciendo una estructura de la información más clara y accesible. Esta organización permite realizar cortes de texto con mayor precisión, facilitando así la obtención de información más pertinente y relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_with_data_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m Markdown(\u001b[43mdocs_with_data_context\u001b[49m[\u001b[32m0\u001b[39m].page_content)\n",
      "\u001b[31mNameError\u001b[39m: name 'docs_with_data_context' is not defined"
     ]
    }
   ],
   "source": [
    "Markdown(docs_with_data_context[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF / DOCX / DOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, vamos a emplear algunos archivos de muestra proporcionados por [Docugami](https://www.docugami.com/). Dichos archivos representan el producto de la extracción de texto de documentos auténticos, en particular, de archivos PDF relativos a contratos de arrendamiento comercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lease_data_dir = pathlib.Path(\"../data/docugami/commercial_lease\")\n",
    "lease_files = list(lease_data_dir.glob(\"*.xml\"))\n",
    "lease_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, carguemos los documentos de muestra y veamos qué propiedades tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La metadata obtenida del documento incluye los siguientes elementos:\n",
    "\n",
    "- `id`, `source_id` y `name`: Estos campos identifican de manera unívoca al documento y al fragmento de texto que se ha extraído de él.\n",
    "- `xpath`: Es el `XPath` correspondiente dentro de la representación XML del documento. Se refiere específicamente al fragmento extraído. Este campo es útil para referenciar directamente las citas del fragmento real dentro del documento XML.\n",
    "- `structure`: Incluye los atributos estructurales del fragmento, tales como `p`, `h1`, `div`, `table`, `td`, entre otros. Es útil para filtrar ciertos tipos de fragmentos, en caso de que el usuario los requiera.\n",
    "- `tag`: Representa la etiqueta semántica para el fragmento. Se genera utilizando diversas técnicas, tanto generativas como extractivas, para determinar el significado del fragmento en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docugami` también posee la capacidad de asistir en la extracción de metadatos específicos para cada `chunk` o fragmento de nuestros documentos. A continuación, se presenta un ejemplo de cómo se extraen y representan estos metadatos:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'xpath': '/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/docset:LeaseParties',\n",
    "    'id': 'v1bvgaozfkak',\n",
    "    'source': 'TruTone Lane 2.docx',\n",
    "    'structure': 'p',\n",
    "    'tag': 'LeaseParties',\n",
    "    'Lease Date': 'April 24 \\n\\n ,',\n",
    "    'Landlord': 'BUBBA CENTER PARTNERSHIP',\n",
    "    'Tenant': 'Truetone Lane LLC',\n",
    "    'Lease Parties': 'Este ACUERDO DE ARRENDAMIENTO DE OFICINA (el \"Contrato\") es celebrado por y entre BUBBA CENTER PARTNERSHIP (\"Arrendador\"), y Truetone Lane LLC, una compañía de responsabilidad limitada de Delaware (\"Arrendatario\").'\n",
    "}\n",
    "```\n",
    "\n",
    "Los metadatos adicionales, como los mostrados arriba, pueden ser extremadamente útiles cuando se implementan `self-retrievers`, los cuales serán explorados adetalle más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar tus documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prefieres utilizar tus propios documentos, puedes cargarlos a través de la interfaz gráfica de [Docugami](https://www.docugami.com/). Una vez cargados, necesitarás asignar cada uno a un `docset`. Un `docset` es un conjunto de documentos que presentan una estructura análoga. Por ejemplo, todos los contratos de arrendamiento comercial por lo general poseen estructuras similares, por lo que pueden ser agrupados en un único `docset`.\n",
    "\n",
    "Después de crear tu `docset`, los documentos cargados serán procesados y estarán disponibles para su acceso mediante la API de `Docugami`.\n",
    "\n",
    "Para recuperar los `ids` de tus documentos y de sus correspondientes `docsets`, puedes ejecutar el siguiente comando:\n",
    "\n",
    "```bash\n",
    "curl --header \"Authorization: Bearer {YOUR_DOCUGAMI_TOKEN}\" \\\n",
    "  https://api.docugami.com/v1preview1/documents\n",
    "```\n",
    "\n",
    "Este comando te facilitará el acceso a la información relevante, optimizando así la administración y organización de tus documentos dentro de `Docugami`.\n",
    "\n",
    "Una vez hayas extraído los `ids` de tus documentos o de los `docsets`, podrás emplearlos para acceder a la información de tus documentos utilizando el `DocugamiLoader` de `Langchain`. Esto te permitirá manipular y gestionar tus documentos dentro de tu aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DocugamiLoader(\n",
    "    docset_id=\"\",\n",
    "    document_ids=None,\n",
    "    file_paths=None,\n",
    ")\n",
    "\n",
    "papers_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_in_the_middle_paper_docs = [\n",
    "    doc for doc in papers_docs if doc.metadata[\"source\"] == \"\"\n",
    "]\n",
    "for doc in lost_in_the_middle_paper_docs:\n",
    "    print(doc.metadata[\"tag\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "platzi_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
